{% set version = "1.7.4" %}
# see github.com/conda-forge/conda-forge.github.io/issues/1059 for naming discussion
{% set faiss_proc_type = "cuda" if cuda_compiler_version != "None" else "cpu" %}

# headers for upstream-folders 'faiss/*.h', 'faiss/{impl,invlists,utils}/*.h',
# see https://github.com/facebookresearch/faiss/blob/v{{ version }}/faiss/CMakeLists.txt;
# gpu adds headers in 'faiss/gpu/*.h', 'faiss/gpu/{impl,utils}/*.(cu)?h'.
# generated by:
# ls faiss/{.,impl,invlists,utils} | grep -E "h$"
# ls faiss/gpu/{.,impl,impl/scan,utils,utils/blockselect,utils/warpselect} | grep -E "h$"
{% set headers = [
    'AutoTune.h', 'Clustering.h', 'IVFlib.h', 'Index.h', 'Index2Layer.h',
    'IndexAdditiveQuantizer.h', 'IndexAdditiveQuantizerFastScan.h', 'IndexBinary.h',
    'IndexBinaryFlat.h', 'IndexBinaryFromFloat.h', 'IndexBinaryHNSW.h', 'IndexBinaryHash.h',
    'IndexBinaryIVF.h', 'IndexFastScan.h', 'IndexFlat.h', 'IndexFlatCodes.h', 'IndexHNSW.h',
    'IndexIDMap.h', 'IndexIVF.h', 'IndexIVFAdditiveQuantizer.h', 'IndexIVFAdditiveQuantizerFastScan.h',
    'IndexIVFFastScan.h', 'IndexIVFFlat.h', 'IndexIVFPQ.h', 'IndexIVFPQFastScan.h', 'IndexIVFPQR.h',
    'IndexIVFSpectralHash.h', 'IndexLSH.h', 'IndexLattice.h', 'IndexNNDescent.h', 'IndexNSG.h',
    'IndexPQ.h', 'IndexPQFastScan.h', 'IndexPreTransform.h', 'IndexRefine.h', 'IndexReplicas.h',
    'IndexRowwiseMinMax.h', 'IndexScalarQuantizer.h', 'IndexShards.h', 'IndexShardsIVF.h',
    'MatrixStats.h', 'MetaIndexes.h', 'MetricType.h', 'VectorTransform.h',
    'clone_index.h', 'index_factory.h', 'index_io.h',
    'impl/AdditiveQuantizer.h', 'impl/AuxIndexStructures.h', 'impl/CodePacker.h',
    'impl/DistanceComputer.h', 'impl/FaissAssert.h', 'impl/FaissException.h', 'impl/HNSW.h',
    'impl/IDSelector.h', 'impl/LocalSearchQuantizer.h', 'impl/LookupTableScaler.h', 'impl/NNDescent.h',
    'impl/NSG.h', 'impl/PolysemousTraining.h', 'impl/ProductAdditiveQuantizer.h',
    'impl/ProductQuantizer-inl.h', 'impl/ProductQuantizer.h', 'impl/Quantizer.h',
    'impl/ResidualQuantizer.h', 'impl/ResultHandler.h', 'impl/ScalarQuantizer.h',
    'impl/ThreadedIndex-inl.h', 'impl/ThreadedIndex.h', 'impl/io.h', 'impl/io_macros.h',
    'impl/kmeans1d.h', 'impl/lattice_Zn.h', 'impl/platform_macros.h', 'impl/pq4_fast_scan.h',
    'impl/simd_result_handlers.h',
    'invlists/BlockInvertedLists.h', 'invlists/DirectMap.h', 'invlists/InvertedLists.h',
    'invlists/InvertedListsIOHook.h',
    'utils/AlignedTable.h', 'utils/Heap.h', 'utils/WorkerThread.h', 'utils/distances.h',
    'utils/extra_distances-inl.h', 'utils/extra_distances.h', 'utils/fp16-fp16c.h',
    'utils/fp16-inl.h', 'utils/fp16.h', 'utils/hamming-inl.h', 'utils/hamming.h',
    'utils/ordered_key_value.h', 'utils/partitioning.h', 'utils/quantize_lut.h',
    'utils/random.h', 'utils/simdlib.h', 'utils/simdlib_avx2.h', 'utils/simdlib_emulated.h',
    'utils/simdlib_neon.h', 'utils/sorting.h', 'utils/utils.h',
] + (not win) * [
    'invlists/OnDiskInvertedLists.h'
] + (cuda_compiler_version != "None") * [
    'gpu/GpuAutoTune.h', 'gpu/GpuCloner.h', 'gpu/GpuClonerOptions.h', 'gpu/GpuDistance.h',
    'gpu/GpuFaissAssert.h', 'gpu/GpuIcmEncoder.h', 'gpu/GpuIndex.h', 'gpu/GpuIndexBinaryFlat.h',
    'gpu/GpuIndexFlat.h', 'gpu/GpuIndexIVF.h', 'gpu/GpuIndexIVFFlat.h', 'gpu/GpuIndexIVFPQ.h',
    'gpu/GpuIndexIVFScalarQuantizer.h', 'gpu/GpuIndicesOptions.h', 'gpu/GpuResources.h',
    'gpu/StandardGpuResources.h',
    'gpu/impl/BinaryDistance.cuh', 'gpu/impl/BinaryFlatIndex.cuh', 'gpu/impl/BroadcastSum.cuh',
    'gpu/impl/Distance.cuh', 'gpu/impl/DistanceUtils.cuh', 'gpu/impl/FlatIndex.cuh',
    'gpu/impl/GeneralDistance.cuh', 'gpu/impl/GpuScalarQuantizer.cuh', 'gpu/impl/IVFAppend.cuh',
    'gpu/impl/IVFBase.cuh', 'gpu/impl/IVFFlat.cuh', 'gpu/impl/IVFFlatScan.cuh',
    'gpu/impl/IVFInterleaved.cuh', 'gpu/impl/IVFPQ.cuh', 'gpu/impl/IVFUtils.cuh',
    'gpu/impl/IcmEncoder.cuh', 'gpu/impl/IndexUtils.h', 'gpu/impl/InterleavedCodes.h',
    'gpu/impl/L2Norm.cuh', 'gpu/impl/L2Select.cuh', 'gpu/impl/PQCodeDistances-inl.cuh',
    'gpu/impl/PQCodeDistances.cuh', 'gpu/impl/PQCodeLoad.cuh',
    'gpu/impl/PQScanMultiPassNoPrecomputed-inl.cuh', 'gpu/impl/PQScanMultiPassNoPrecomputed.cuh',
    'gpu/impl/PQScanMultiPassPrecomputed.cuh', 'gpu/impl/RemapIndices.h', 'gpu/impl/VectorResidual.cuh',
    'gpu/impl/scan/IVFInterleavedImpl.cuh',
    'gpu/utils/BlockSelectKernel.cuh', 'gpu/utils/Comparators.cuh',
    'gpu/utils/ConversionOperators.cuh', 'gpu/utils/CopyUtils.cuh', 'gpu/utils/DeviceDefs.cuh',
    'gpu/utils/DeviceTensor-inl.cuh', 'gpu/utils/DeviceTensor.cuh', 'gpu/utils/DeviceUtils.h',
    'gpu/utils/DeviceVector.cuh', 'gpu/utils/Float16.cuh', 'gpu/utils/HostTensor-inl.cuh',
    'gpu/utils/HostTensor.cuh', 'gpu/utils/Limits.cuh', 'gpu/utils/LoadStoreOperators.cuh',
    'gpu/utils/MathOperators.cuh', 'gpu/utils/MatrixMult-inl.cuh', 'gpu/utils/MatrixMult.cuh',
    'gpu/utils/MergeNetworkBlock.cuh', 'gpu/utils/MergeNetworkUtils.cuh',
    'gpu/utils/MergeNetworkWarp.cuh', 'gpu/utils/NoTypeTensor.cuh', 'gpu/utils/Pair.cuh',
    'gpu/utils/PtxUtils.cuh', 'gpu/utils/ReductionOperators.cuh', 'gpu/utils/Reductions.cuh',
    'gpu/utils/Select.cuh', 'gpu/utils/StackDeviceMemory.h', 'gpu/utils/StaticUtils.h',
    'gpu/utils/Tensor-inl.cuh', 'gpu/utils/Tensor.cuh', 'gpu/utils/ThrustUtils.cuh',
    'gpu/utils/Timer.h', 'gpu/utils/Transpose.cuh', 'gpu/utils/WarpPackedBits.cuh',
    'gpu/utils/WarpSelectKernel.cuh', 'gpu/utils/WarpShuffles.cuh',
    'gpu/utils/blockselect/BlockSelectImpl.cuh', 'gpu/utils/warpselect/WarpSelectImpl.cuh'
] %}

package:
  name: faiss-split
  version: {{ version }}

source:
  url: https://github.com/facebookresearch/faiss/archive/v{{ version }}.tar.gz
  sha256: d9a7b31bf7fd6eb32c10b7ea7ff918160eed5be04fe63bb7b4b4b5f2bbde01ad
  patches:
    - patches/0001-use-c-17.patch
    # downgrade to C++14 for resp. CUDA 11.1 on linux
    - patches/0001-use-c-14.patch  # [linux and (cuda_compiler_version == "11.1")]
    # adapt header target directory for faiss_avx2
    - patches/0002-adapt-header-target-directory-to-outputname.patch
    # patch for avoiding crash in GPU test suite on windows
    - patches/0003-skip-test_stress-for-GPU-on-windows.patch
    # enable building libfaiss_avx2 without libfaiss
    - patches/0004-enable-building-libfaiss_avx2-without-libfaiss.patch
    # increase tolerance for test that occasionally fails marginally
    - patches/0005-increase-tolerance-for-marginally-failing-test.patch
    # add /bigobj on windows to avoid: "fatal error C1128: number of sections exceeded object file format limit"
    - patches/0006-add-bigobj-to-swigfaiss-compile-options-on-windows.patch
    # patch out use of non-portable GCC SIMD extension
    - patches/0007-dont-use-GCC-extension-that-doesn-t-work-on-MSVC.patch
    # the _mm_prefetch of MSVC is stricter about which types it accepts
    - patches/0008-ensure-_mm_prefetch-gets-char-not-float-as-first-arg.patch
    # due to switch to ninja
    - patches/0009-no-more-Release-subfolder.patch
    # more fixes
    - patches/0010-fix-index-type-in-openmp-loop.patch
    - patches/0011-add-missing-headers.patch
    # relax test: https://github.com/facebookresearch/faiss/commit/391601dc3f4793e10aaf9d21f5bbe2a94ecf4f50
    # Relaxed a bit more due to the issue occasionally coming back
    - patches/0012-relax_test_ivf_train_2level.patch

build:
  number: 0
  # some MKL issue for py 3.8 on osx-64
  skip: true  # [py<38 or (py==38 and osx and x86_64)]
  # Too many tests are failing on s390x.
  skip: true  # [linux and (s390x or ppc64le)]

requirements:
  build:
    - {{ compiler('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}  # [cuda_compiler_version != "None"]
    - patch  # [not win]
    - m2-patch  # [win]
    - cmake
    - ninja
    # libgomp is required for linux-64 alongside intel-omp.
    - libgomp                 # [linux]
    # llvm-openmp is required only for osx
    - llvm-openmp  14.0.6     # [osx]
  host:
    - python
    # OpenBLAS + libgomp for osx-arm64, and linux-s390x and linux-ppc64le, linux-aarch64.
    - openblas 0.3.21         # [s390x or ppc64le or aarch64 or arm64]
    # MKL for win, linux-64, and osx-64. All require intel-openmp.
    - mkl 2023.1.0            # [not (s390x or ppc64le or aarch64 or arm64)]
    - intel-openmp            # [linux64 or win]
    # mkl-devel seems to be required for win.
    - mkl-devel 2023.1.0      # [win]

outputs:
  # A meta-package to select CPU or GPU build for faiss.
  - name: faiss-proc
    version: 1.0.0
    build:
      # some MKL issue for py 3.8 on osx-64
      skip: true  # [py==38 and osx and x86_64]
      string: {{ faiss_proc_type }}
    requirements:

    test:
      commands:
        - exit 0

  # build two separate C++ libs, one generic, and one for AVX2 on x86_64
  {% set CF_FAISS_BUILD_OPTS = ["generic"] %}
  {% set CF_FAISS_BUILD_OPTS = ["avx2"] + CF_FAISS_BUILD_OPTS %}  # [x86_64]
  {% for CF_FAISS_BUILD in CF_FAISS_BUILD_OPTS %}
  # order libfaiss last in loop due to conda/conda-build#4090; libfaiss-avx2
  # is only used for faiss and not important enough to work-around for this bug
  {% if CF_FAISS_BUILD == "generic" %}
  - name: libfaiss
  {% else %}
  - name: libfaiss-avx2
  {% endif %}
  {% set libext = "_avx2" if CF_FAISS_BUILD == "avx2" else "" %}
    # only one main build script build-lib.{bat|sh}, with the only difference
    # through CF_FAISS_BUILD={generic,avx2} that's set in the wrappers
    script: build-lib-{{ CF_FAISS_BUILD }}.sh   # [not win]
    script: build-lib-{{ CF_FAISS_BUILD }}.bat  # [win]
    build:
      # some MKL issue for py 3.8 on osx-64
      skip: true  # [py==38 and osx and x86_64]
      string: "h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}"                                                  # [cuda_compiler_version == "None"]
      string: "cuda{{ cuda_compiler_version|replace(".", "") }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}"  # [cuda_compiler_version != "None"]
      run_exports:
        # faiss follows SemVer, so restrict packages built with libfaiss to use
        # at least the same version at runtime, but below the next major version.
        - libfaiss{{ libext }} >={{ version }},<2
        # additionally, we need to ensure matching proc-type
        - libfaiss{{ libext }} =*=*_{{ faiss_proc_type }}
    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}  # [cuda_compiler_version != "None"]
        - cmake
        - ninja
        # libgomp is required for linux-64 alongside intel-omp.
        - libgomp                 # [linux]
        # llvm-openmp is required only for osx
        - llvm-openmp  14.0.6     # [osx]
      host:
        - python
        # OpenBLAS + libgomp for osx-arm64, and linux-s390x and linux-ppc64le, linux-aarch64.
        - openblas 0.3.21         # [s390x or ppc64le or aarch64 or arm64]
        # MKL for win, linux-64, and osx-64. All require intel-openmp.
        - mkl 2023.1.0            # [not (s390x or ppc64le or aarch64 or arm64)] 
        - intel-openmp            # [linux64 or win]
        # mkl-devel seems to be required for win.
        - mkl-devel 2023.1.0      # [win]
      run:
        # Run dependencies follow the same logic as above.
        - libopenblas            # [s390x or ppc64le or aarch64 or arm64] 
        - mkl 2023.1.*           # [not (s390x or ppc64le or aarch64 or arm64)]
        - {{ pin_compatible('intel-openmp') }}  # [linux64 or win]
        - llvm-openmp  # [osx]
      run_constrained:
        - faiss-cpu ==9999999999  # [cuda_compiler_version != "None"]
        - faiss-gpu ==9999999999  # [cuda_compiler_version == "None"]
        - faiss-proc =*={{ faiss_proc_type }}

    test:
      commands:
        # shared
        - test -f $PREFIX/lib/libfaiss{{ libext }}.so               # [linux]
        - test -f $PREFIX/lib/libfaiss{{ libext }}.dylib            # [osx]
        - if not exist %LIBRARY_BIN%\faiss{{ libext }}.dll exit 1   # [win]
        # On windows, faiss.lib is an "import library";
        # Deleting it breaks the faiss-builds
        - if not exist %LIBRARY_LIB%\faiss{{ libext }}.lib exit 1   # [win]

        # absence of static libraries
        - test ! -f $PREFIX/lib/libfaiss{{ libext }}.a              # [not win]

        # headers
        {% for each_header in headers %}
        - test -f $PREFIX/include/faiss{{ libext }}/{{ each_header }} || (echo "{{ each_header }} not found" && exit 1)  # [unix]
        - if not exist %LIBRARY_INC%\faiss{{ libext }}\{{ "\\".join(each_header.split("/")) }} exit 1                    # [win]
        {% endfor %}
  {% endfor %}

  - name: faiss
    script: build-pkg.sh          # [not win]
    script: build-pkg.bat         # [win]
    build:
      # some MKL issue for py 3.8 on osx-64
      skip: true  # [py==38 and osx and x86_64]
      string: "py{{ PY_VER|replace(".","") }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}"                                                  # [cuda_compiler_version == "None"]
      string: "py{{ PY_VER|replace(".","") }}cuda{{ cuda_compiler_version|replace(".", "") }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}"  # [cuda_compiler_version != "None"]
    requirements:
      build:
        - {{ compiler('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}  # [cuda_compiler_version != "None"]
        - swig
        - cmake
        - ninja
        # libgomp is required for linux-64 alongside intel-omp.
        - libgomp                 # [linux]
        # llvm-openmp is required only for osx
        - llvm-openmp  14.0.6     # [osx]
      host:
        - python
        - wheel
        - packaging
        - setuptools
        - pip
        - numpy  {{ numpy }}
        # OpenBLAS + libgomp for osx-arm64, and linux-s390x and linux-ppc64le, and linux-aarch64. 
        - openblas 0.3.21         # [s390x or ppc64le or aarch64 or arm64]     
        # MKL for win, linux-64, and osx-64. All require intel-openmp.
        - mkl 2023.1.0            # [not (s390x or ppc64le or aarch64 or arm64)] 
        # mkl-devel seems to be required for win.
        - mkl-devel 2023.1.0      # [win]
        - libfaiss ={{ version }}=*_{{ faiss_proc_type }}
        - libfaiss-avx2 ={{ version }}=*_{{ faiss_proc_type }}  # [x86_64]
      run:
        - python
        - libfaiss ={{ version }}=*_{{ faiss_proc_type }}
        - libfaiss-avx2 ={{ version }}=*_{{ faiss_proc_type }}  # [x86_64]
        - {{ pin_compatible('numpy') }}
        - libopenblas            # [s390x or ppc64le or aarch64 or arm64]
        - mkl 2023.1.*           # [not (s390x or ppc64le or aarch64 or arm64)]
        - llvm-openmp            # [osx]
      run_constrained:
        - faiss-cpu ==9999999999  # [cuda_compiler_version != "None"]
        - faiss-gpu ==9999999999  # [cuda_compiler_version == "None"]
        - faiss-proc =*={{ faiss_proc_type }}

    test:
      requires:
        - scipy
        - pytest
      files:
        - test-pkg.bat
        - test-pkg.sh
      source_files:
        - tests/
      imports:
        - faiss
      commands:
        {% set skip = "_not_a_real_test" %}
        # flaky double encoding test that occasionally fails
        {% set skips = "test_RQ6x8" %}
        # TestComputeGT switches between CPU & GPU implementation depending on availability;
        # GPU device detection (on win) currently seems broken in CUDA, and segfaults the test suite
        {% set skips = skips + " or TestComputeGT" %}                                      # [win]
        # test relies on linux-isms
        {% set skips = skips + " or (test_contrib and test_checkpoint)" %}                 # [win]
        # two failing tests on on aarch (test_index_accuracy & test_index_accuracy2)
        {% set skips = skips + " or (test_residual_quantizer and test_index_accuracy)" %}  # [aarch64 or ppc64le]
        # test_ivf_train_2level fails randomly on linux-64. Upstream recently increased the threshold. Tried that, but still failing occasionally.
        {% set skips = skips + " or test_ivf_train_2level" %}  # [linux and x86_64]
        # Speed tests on aarch64 and arm64 ocassionally take a bit longer than threshold, e.g: (AssertionError: 0.10208797454833984 not less than 0.09767270088195801)
        # These use openblas while upstream mentions MKL ensures the best speed.
        {% set skips = skips + " or test_PQ4_speed" %}  # [aarch64 or arm64] 
        # the linux & windows CI agents support AVX2 (OSX doesn't yet), so by default,
        # we expect faiss will load the library with AVX2-support, see
        # https://github.com/facebookresearch/faiss/blob/v1.7.1/faiss/python/loader.py#L52-L66
        - export HAS_AVX2=YES && export SKIPS="({{ skips }})" && ./test-pkg.sh  # [linux and x86_64]
        - export HAS_AVX2=NO  && export SKIPS="({{ skips }})" && ./test-pkg.sh  # [osx or (linux and not x86_64)]
        - set "HAS_AVX2=YES"  && set "SKIPS=({{ skips }})"    && test-pkg.bat   # [win]

        # running the following test requires an actual GPU device, which is not available in CI
        # - pytest faiss/gpu/test/

  # for compatibility with (& ease of migration from) existing packages in the pytorch channel
  - name: faiss-cpu
    build:
      skip: true  # [cuda_compiler_version != "None"]
      # some MKL issue for py 3.8 on osx-64
      skip: true  # [py==38 and osx and x86_64]
    requirements:
      run:
        - faiss ={{ version }}=*_cpu
    test:
      imports:
        - faiss

  - name: faiss-gpu
    build:
      skip: true  # [cuda_compiler_version == "None"]
      # some MKL issue for py 3.8 on osx-64
      skip: true  # [py==38 and osx and x86_64]
    requirements:
      run:
        - faiss ={{ version }}=*_cuda
    test:
      imports:
        - faiss

about:
  home: https://github.com/facebookresearch/faiss
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: 'A library for efficient similarity search and clustering of dense vectors.'
  description: |
    Faiss is a library for efficient similarity search and clustering of dense vectors.
    It contains algorithms that search in sets of vectors of any size, up to ones that
    possibly do not fit in RAM. It also contains supporting code for evaluation and
    parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy.
    Some of the most useful algorithms are implemented on the GPU. It is developed by
    Facebook AI Research.

    For best performance, the maintainers of the package
    [recommend](https://github.com/conda-forge/staged-recipes/pull/11337#issuecomment-623718460)
    using the MKL implementation of blas/lapack.
  doc_url: https://faiss.ai/
  dev_url: https://github.com/facebookresearch/faiss

extra:
  recipe-maintainers:
    - h-vetinari
